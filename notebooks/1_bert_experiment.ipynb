{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from pandas import read_csv\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from transformers import BertConfig, BertTokenizer, BertModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "BASE_DIR = Path('..')\n",
    "PRETRAINED_MODEL_DIR = BASE_DIR / 'models' / 'dna-bert' / f'{K}-new-12w-0'\n",
    "DATASET_DIR = BASE_DIR / 'data' / 'processed' / 'H.sapiens495' / f'kmer_{K}'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ..\\models\\dna-bert\\3-new-12w-0 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "pre_model = BertModel.from_pretrained(PRETRAINED_MODEL_DIR, config=config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data_dir: Path, dataset: str, set_type: str, k: str, tokenizer: BertTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        data = read_csv(data_dir / 'processed' / dataset / f'kmer_text_{k}' / f'{set_type}.csv')\n",
    "\n",
    "        self.sequences = data['x1']\n",
    "        self.labels = data['label']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokenized_seq = self.tokenizer.encode_plus(self.sequences.iloc[index], None, return_token_type_ids=True)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(tokenized_seq['input_ids'], dtype=torch.long),\n",
    "            torch.tensor(tokenized_seq['attention_mask'], dtype=torch.long),\n",
    "            torch.tensor(tokenized_seq['token_type_ids'], dtype=torch.long),\n",
    "            torch.tensor(self.labels.iloc[index], dtype=torch.long),\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "valid_data = KmerDataset(Path('..') / 'data', 'H.sapiens100', 'valid', K, tokenizer)\n",
    "train_data = KmerDataset(Path('..') / 'data', 'H.sapiens100', 'train', K, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0)\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_data, batch_size=16, shuffle=True, num_workers=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class PsiSequenceClassifier(nn.Module):\n",
    "    def __init__(self, pre_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = pre_model\n",
    "        # self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        x = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_states = x[0]\n",
    "        x = hidden_states[:, 0]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 49\u001B[0m\n\u001B[0;32m     47\u001B[0m EPOCHS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[1;32m---> 49\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalid\u001B[39m(model, testing_loader):\n\u001B[0;32m     52\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n",
      "Cell \u001B[1;32mIn[31], line 36\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(epoch, training_loader)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# if _%500==0:\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m#     loss_step = tr_loss/nb_tr_steps\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m#     accu_step = (n_correct*100)/nb_tr_examples\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m#     print(f\"Training Loss per 500 steps: {loss_step}\")\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m#     print(f\"Training Accuracy per 500 steps: {accu_step}\")\u001B[39;00m\n\u001B[0;32m     35\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 36\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# # When using GPU\u001B[39;00m\n\u001B[0;32m     38\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\rna_modification\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\rna_modification\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "    device = 'cpu'\n",
    "    model = PsiSequenceClassifier(pre_model)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=10e-4)\n",
    "\n",
    "    def calculate_accuracy(y_pred, y_true):\n",
    "        n_correct = (y_pred == y_true).sum().item()\n",
    "        return n_correct\n",
    "\n",
    "\n",
    "    def train(epoch, training_loader):\n",
    "        tr_loss = 0\n",
    "        n_correct = 0\n",
    "        nb_tr_steps = 0\n",
    "        nb_tr_examples = 0\n",
    "        model.train()\n",
    "        for index, data in tqdm(enumerate(training_loader, 0)):\n",
    "            ids, mask, token_type_ids, labels = data\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calculate_accuracy(big_idx, labels)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += labels.size(0)\n",
    "\n",
    "            # if _%500==0:\n",
    "            #     loss_step = tr_loss/nb_tr_steps\n",
    "            #     accu_step = (n_correct*100)/nb_tr_examples\n",
    "            #     print(f\"Training Loss per 500 steps: {loss_step}\")\n",
    "            #     print(f\"Training Accuracy per 500 steps: {accu_step}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # When using GPU\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "        print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "\n",
    "    EPOCHS = 10\n",
    "    for epoch in range(EPOCHS):\n",
    "        train(epoch, train_data_loader)\n",
    "\n",
    "    def valid(model, testing_loader):\n",
    "        model.eval()\n",
    "        n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "        with torch.no_grad():\n",
    "            for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "                ids, mask, token_type_ids, labels = data\n",
    "\n",
    "                # ids = data['ids'].to(device, dtype = torch.long)\n",
    "                # mask = data['mask'].to(device, dtype = torch.long)\n",
    "                # token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "                # targets = data['targets'].to(device, dtype = torch.long)\n",
    "                outputs = model(ids, mask, token_type_ids)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                tr_loss += loss.item()\n",
    "                big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "                n_correct += calculate_accuracy(big_idx, labels)\n",
    "\n",
    "                nb_tr_steps += 1\n",
    "                nb_tr_examples += labels.size(0)\n",
    "\n",
    "                if _ % 5000==0:\n",
    "                    loss_step = tr_loss/nb_tr_steps\n",
    "                    accu_step = (n_correct*100)/nb_tr_examples\n",
    "                    print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                    print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "        epoch_loss = tr_loss/nb_tr_steps\n",
    "        epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "        print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "        return epoch_accu\n",
    "    acc = valid(model, valid_data_loader)\n",
    "    print(\"Accuracy on validation data = %0.2f%%\" % acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
