{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from sklearn.metrics import auc, RocCurveDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from matplotlib.pyplot import subplots, savefig, close\n",
    "from numpy import linspace, interp, mean, std, minimum, maximum\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_DIR = Path('https://pseudouridine.s3.ap-south-1.amazonaws.com/data')\n",
    "else:\n",
    "    DATA_DIR = Path('..') / 'data'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPECIES = [\n",
    "    'H.Sapiens100',\n",
    "    'H.Sapiens495',\n",
    "    'M.musculus472',\n",
    "    'S.cerevisiae100',\n",
    "    'S.cerevisiae314'\n",
    "]\n",
    "\n",
    "ENCODINGS = [\n",
    "    'anf',\n",
    "    'binary',\n",
    "    'cksnap',\n",
    "    'dnc',\n",
    "    'eiip',\n",
    "    'enac',\n",
    "    'kmer',\n",
    "    'knc',\n",
    "    'nac',\n",
    "    'ncp',\n",
    "    'nd',\n",
    "    'psdp',\n",
    "    'pse_eiip',\n",
    "    'pse_knc',\n",
    "    'psnp',\n",
    "    'pstp',\n",
    "    'rc_kmer'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "ML_MODELS = {\n",
    "    'svc': lambda params: SVC(**params),\n",
    "    'linear_svc': lambda params: LinearSVC(**params),\n",
    "    'logistic': lambda params: LogisticRegression(**params),\n",
    "    'knn': lambda params: KNeighborsClassifier(**params),\n",
    "    'random_forest': lambda params: RandomForestClassifier(**params),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def visualize_cv_roc(results):\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = linspace(0, 1, 100)\n",
    "    fig, ax = subplots(figsize=(7.5, 7.5))\n",
    "\n",
    "    for index, (y_true, y_pred) in enumerate(results):\n",
    "        viz = RocCurveDisplay.from_predictions(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            name=f'ROC fold {index}',\n",
    "            alpha=0.3,\n",
    "            lw=1,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        interp_tpr = interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "    mean_tpr = mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = std(aucs)\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    std_tpr = std(tprs, axis=0)\n",
    "    tprs_upper = minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        tprs_lower,\n",
    "        tprs_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.set(\n",
    "        xlim=[-0.05, 1.05],\n",
    "        ylim=[-0.05, 1.05],\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=f\"Mean ROC curve with variability\\n(Positive label')\",\n",
    "    )\n",
    "    ax.axis(\"square\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def cv_classification_report(results):\n",
    "    score_sum = classification_report(results[0][0], results[0][1], output_dict=True, zero_division=0)\n",
    "    for result in results[1:]:\n",
    "        score = classification_report(result[0], result[1], output_dict=True, zero_division=0)\n",
    "\n",
    "        score_sum['0']['precision'] += score['0']['precision']\n",
    "        score_sum['0']['recall'] += score['0']['recall']\n",
    "        score_sum['0']['f1-score'] += score['0']['f1-score']\n",
    "        score_sum['0']['support'] += score['0']['support']\n",
    "\n",
    "        score_sum['1']['precision'] += score['1']['precision']\n",
    "        score_sum['1']['recall'] += score['1']['recall']\n",
    "        score_sum['1']['f1-score'] += score['1']['f1-score']\n",
    "        score_sum['1']['support'] += score['1']['support']\n",
    "\n",
    "        score_sum['accuracy'] += score['accuracy']\n",
    "\n",
    "        score_sum['macro avg']['precision'] += score['macro avg']['precision']\n",
    "        score_sum['macro avg']['recall'] += score['macro avg']['recall']\n",
    "        score_sum['macro avg']['f1-score'] += score['macro avg']['f1-score']\n",
    "        score_sum['macro avg']['support'] += score['macro avg']['support']\n",
    "\n",
    "        score_sum['weighted avg']['precision'] += score['weighted avg']['precision']\n",
    "        score_sum['weighted avg']['recall'] += score['weighted avg']['recall']\n",
    "        score_sum['weighted avg']['f1-score'] += score['weighted avg']['f1-score']\n",
    "        score_sum['weighted avg']['support'] += score['weighted avg']['support']\n",
    "\n",
    "    score_sum['0']['precision'] /= len(results)\n",
    "    score_sum['0']['recall'] /= len(results)\n",
    "    score_sum['0']['f1-score'] /= len(results)\n",
    "    score_sum['0']['support'] /= len(results)\n",
    "\n",
    "    score_sum['1']['precision'] /= len(results)\n",
    "    score_sum['1']['recall'] /= len(results)\n",
    "    score_sum['1']['f1-score'] /= len(results)\n",
    "    score_sum['1']['support'] /= len(results)\n",
    "\n",
    "    score_sum['accuracy'] /= len(results)\n",
    "\n",
    "    score_sum['macro avg']['precision'] /= len(results)\n",
    "    score_sum['macro avg']['recall'] /= len(results)\n",
    "    score_sum['macro avg']['f1-score'] /= len(results)\n",
    "    score_sum['macro avg']['support'] /= len(results)\n",
    "\n",
    "    score_sum['weighted avg']['precision'] /= len(results)\n",
    "    score_sum['weighted avg']['recall'] /= len(results)\n",
    "    score_sum['weighted avg']['f1-score'] /= len(results)\n",
    "    score_sum['weighted avg']['support'] /= len(results)\n",
    "\n",
    "    return score_sum\n",
    "\n",
    "\n",
    "def train_model_cv(model, dataset, splits=10):\n",
    "    data = read_csv(dataset / 'all.csv')\n",
    "\n",
    "    x = data.drop('label', axis=1)\n",
    "    y = data['label']\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits)\n",
    "\n",
    "    results = []\n",
    "    for fold, (train, test) in enumerate(k_fold.split(x, y)):\n",
    "        model.fit(x.iloc[train], y.iloc[train])\n",
    "\n",
    "        y_true = y.iloc[test]\n",
    "        y_pred = model.predict(x.iloc[test])\n",
    "\n",
    "        results.append((y_true, y_pred))\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def train_model(model, dataset):\n",
    "    data1 = read_csv(dataset / 'train.csv')\n",
    "    data2 = read_csv(dataset / 'valid.csv')\n",
    "\n",
    "    data = concat([data1, data2])\n",
    "\n",
    "    x = data.drop('label', axis=1)\n",
    "    y = data['label']\n",
    "\n",
    "    model.fit(x, y)\n",
    "\n",
    "\n",
    "def simple_classification_report(model, dataset):\n",
    "    data = read_csv(dataset / 'test.csv')\n",
    "\n",
    "    x = data.drop('label', axis=1)\n",
    "    y = data['label']\n",
    "\n",
    "    return classification_report(model.predict(x), y, output_dict=True, zero_division=0)\n",
    "\n",
    "\n",
    "def visualize_roc(model, dataset):\n",
    "    data = read_csv(dataset / 'test.csv')\n",
    "\n",
    "    x = data.drop('label', axis=1)\n",
    "    y = data['label']\n",
    "\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        model.predict(x),\n",
    "        y,\n",
    "        name=f\"ROC\",\n",
    "        color=\"darkorange\",\n",
    "    )\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"AUC = 0.5\")\n",
    "    plt.axis(\"square\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def perform_experiment(specie, encoding, model_name, model_params=None, cv=True, cv_splits=10, save_dir=None):\n",
    "    model = ML_MODELS[model_name](model_params if model_params is not None else dict())\n",
    "    dataset = DATA_DIR / 'processed' / specie / encoding\n",
    "\n",
    "    if cv:\n",
    "        results = train_model_cv(model, dataset, cv_splits)\n",
    "        figure = visualize_cv_roc(results)\n",
    "        report = cv_classification_report(results)\n",
    "\n",
    "        if save_dir is None:\n",
    "            figure.show()\n",
    "            print(DataFrame(report).transpose())\n",
    "        else:\n",
    "            save_dir = Path(save_dir)\n",
    "            save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            exp_name = f'{model_name}({specie})[{encoding}]_cv={cv_splits}'\n",
    "            print()\n",
    "            figure.savefig(save_dir / f'{exp_name}.svg')\n",
    "            close()\n",
    "            DataFrame(report).transpose().to_csv(save_dir / f'{exp_name}.csv')\n",
    "    else:\n",
    "        train_model(model, dataset)\n",
    "        visualize_roc(model, dataset)\n",
    "        report = simple_classification_report(model, dataset)\n",
    "\n",
    "        if save_dir is None:\n",
    "            print(DataFrame(report).transpose())\n",
    "        else:\n",
    "            save_dir = Path(save_dir)\n",
    "            save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            exp_name = f'{model_name}({specie})[{encoding}]'\n",
    "            savefig(save_dir / f'{exp_name}.svg')\n",
    "            close()\n",
    "            DataFrame(report).transpose().to_csv(save_dir / f'{exp_name}.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "perform_experiment(SPECIES[1], ENCODINGS[0], 'svc', cv=False, save_dir='../results')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
